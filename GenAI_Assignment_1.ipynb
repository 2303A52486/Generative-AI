{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzbL3dYUTsdkbwtRUM5Bst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52486/Generative-AI/blob/main/GenAI_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V.Abhinav(2303A52486)"
      ],
      "metadata": {
        "id": "cfHNyBTNsq3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write Python code from scratch to find error metrics of deep learning model. Actual values and deep learning model predicted values are shown in Table-1. Also compare the results with the outcomes of libraries"
      ],
      "metadata": {
        "id": "74PX3T9DuK0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvopQEv-qGad",
        "outputId": "1ad26993-4323-47fb-80a4-ae0ba24ca518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "R-squared (R²): 0.99877\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "Actual = np.array([20, 30, 40, 50, 60])\n",
        "Pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "# Calculating Manually\n",
        "\n",
        "# 1. MAE\n",
        "mae = np.mean(np.abs(Actual - Pred))\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# 2. MSE\n",
        "mse = np.mean((Actual - Pred) ** 2)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# 3. R-Squared\n",
        "ss_total = np.sum((Actual - np.mean(Actual)) ** 2)\n",
        "ss_residual = np.sum((Actual - Pred) ** 2)\n",
        "r2 = 1 - (ss_residual / ss_total)\n",
        "print(f\"R-squared (R²): {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calulating with scikit-learn Library\n",
        "\n",
        "# MAE using sklearn\n",
        "mae_sk = mean_absolute_error(Actual, Pred)\n",
        "print(f\"MAE (sklearn): {mae_sk}\")\n",
        "\n",
        "# MSE using sklearn\n",
        "mse_sk = mean_squared_error(Actual, Pred)\n",
        "print(f\"MSE (sklearn): {mse_sk}\")\n",
        "\n",
        "# R² using sklearn\n",
        "r2_sk = r2_score(Actual, Pred)\n",
        "print(f\"R-squared (sklearn): {r2_sk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_--4tIBmujXA",
        "outputId": "b79692bb-778a-4329-b5c3-c05221618ad0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE (sklearn): 0.4600000000000016\n",
            "MSE (sklearn): 0.24600000000000147\n",
            "R-squared (sklearn): 0.99877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries"
      ],
      "metadata": {
        "id": "nyvUlRfdBNIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "y_actual = [0, 1, 1, 2, 0, 0, 1, 0, 2, 0, 2, 1, 1, 2, 2]\n",
        "y_pred = [0, 1, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 2, 2, 2]\n",
        "\n",
        "# A function to compute evaluation metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    unique_classes = np.unique(y_true)\n",
        "    metrics = {}\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
        "    metrics['accuracy'] = accuracy\n",
        "\n",
        "    precision = {}\n",
        "    recall = {}\n",
        "    f1 = {}\n",
        "    for cls in unique_classes:\n",
        "        tp = np.sum((np.array(y_true) == cls) & (np.array(y_pred) == cls))\n",
        "        fp = np.sum((np.array(y_true) != cls) & (np.array(y_pred) == cls))\n",
        "        fn = np.sum((np.array(y_true) == cls) & (np.array(y_pred) != cls))\n",
        "\n",
        "        precision[cls] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall[cls] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1[cls] = (2 * precision[cls] * recall[cls]) / (precision[cls] + recall[cls]) if (precision[cls] + recall[cls]) > 0 else 0\n",
        "\n",
        "    metrics['precision'] = precision\n",
        "    metrics['recall'] = recall\n",
        "    metrics['f1_score'] = f1\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Compute metrics from scratch\n",
        "custom_metrics = compute_metrics(y_actual, y_pred)\n",
        "\n",
        "# Compute metrics using sklearn\n",
        "sklearn_metrics = {\n",
        "    'accuracy': accuracy_score(y_actual, y_pred),\n",
        "    'precision': precision_score(y_actual, y_pred, average=None, zero_division=0).tolist(),\n",
        "    'recall': recall_score(y_actual, y_pred, average=None, zero_division=0).tolist(),\n",
        "    'f1_score': f1_score(y_actual, y_pred, average=None, zero_division=0).tolist()\n",
        "}\n",
        "\n",
        "# Displaying the results\n",
        "print(\"Metrics computed Manually:\")\n",
        "print(custom_metrics)\n",
        "print(\"\\nMetrics computed using sklearn:\")\n",
        "print(sklearn_metrics)"
      ],
      "metadata": {
        "id": "Lwivi2QyBbbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4a45b6-c19f-44ba-bd89-ddc9ec887f0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics computed Manually:\n",
            "{'accuracy': 0.6, 'precision': {0: 0.75, 1: 0.6666666666666666, 2: 0.5}, 'recall': {0: 0.6, 1: 0.4, 2: 0.8}, 'f1_score': {0: 0.6666666666666665, 1: 0.5, 2: 0.6153846153846154}}\n",
            "\n",
            "Metrics computed using sklearn:\n",
            "{'accuracy': 0.6, 'precision': [0.75, 0.6666666666666666, 0.5], 'recall': [0.6, 0.4, 0.8], 'f1_score': [0.6666666666666666, 0.5, 0.6153846153846154]}\n"
          ]
        }
      ]
    }
  ]
}